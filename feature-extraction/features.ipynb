{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5370a25e-de49-49af-b0f4-680059bf7afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from matplotlib import animation\n",
    "from matplotlib.animation import FFMpegWriter\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92747a6-1710-4bcc-8e9b-483725294ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_coord_line(parts):\n",
    "    if len(parts) != 3:\n",
    "        return False\n",
    "    try:\n",
    "        int(parts[0])\n",
    "        float(parts[1])\n",
    "        float(parts[2])\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "        \n",
    "def _is_demand_line(parts):\n",
    "    if len(parts) != 2:\n",
    "        return False\n",
    "    try:\n",
    "        int(parts[0]); int(parts[1])\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "class Instance:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        \n",
    "        # the name of the instance \n",
    "        self.name = None\n",
    "        \n",
    "        # the variables below can be read directly from the \n",
    "        self.optimum = None # the optimum value\n",
    "        self.num_of_vehicles = None\n",
    "        self.dimension = None\n",
    "        self.num_of_customers = None\n",
    "        self.num_of_stations = None # the number of recharging stations \n",
    "        self.capacity = None # the maximum capacity of vehicle\n",
    "        self.battery_capacity = None # the battery capacity of EV\n",
    "        self.energy_consumption = None # the energy consumption rate\n",
    "        self.edge_weight_format = None\n",
    "        self.node_coordinates = {} # dictionary {'0': [145, 215], ...}\n",
    "        self.demands = {} # {'0': 0, '1':1100}\n",
    "        self.station_list = [] # ['22', '23', '24'] \n",
    "        self.depot_index = None # 1\n",
    "        \n",
    "        # the variables below can be gotten after processing\n",
    "        self.actual_problem_size = None # Total number of customers, charging stations and depot\n",
    "        self.distance_matrix = None\n",
    "        \n",
    "        self.read_problem()\n",
    "            \n",
    "    def read_problem(self):   \n",
    "        self.name = os.path.splitext(self.filename.split('/')[-1])[0]\n",
    "        with open(self.filename, 'rt', encoding='utf-8', newline='') as f:\n",
    "            line = f.readline().strip()\n",
    "            while line:\n",
    "                if line.startswith('OPTIMAL_VALUE:'):\n",
    "                    # 兼容：OPTIMAL_VALUE: 167575 (Best Known)\n",
    "                    # 以及旧版：OPTIMAL_VALUE: 167575\n",
    "                    m = re.search(r'OPTIMAL_VALUE:\\s*([-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?)', line)\n",
    "                    if m:\n",
    "                        self.optimum = float(m.group(1))\n",
    "                    else:\n",
    "                        raise Exception(f\"Invalid OPTIMAL_VALUE line: {line}\")\n",
    "                elif line.startswith('VEHICLES:'):\n",
    "                    self.num_of_vehicles = int(line.split()[-1])\n",
    "                elif line.startswith('DIMENSION:'):\n",
    "                    self.dimension = int(line.split()[-1])\n",
    "                elif line.startswith('STATIONS:'):\n",
    "                    self.num_of_stations = int(line.split()[-1])\n",
    "                elif line.startswith('CAPACITY:'):\n",
    "                    self.capacity = int(line.split()[-1])\n",
    "                elif line.startswith('ENERGY_CAPACITY:'):\n",
    "                    self.battery_capacity = int(line.split()[-1])       \n",
    "                elif line.startswith('ENERGY_CONSUMPTION:'):\n",
    "                    self.energy_consumption = float(line.split()[-1])\n",
    "                elif line.startswith('EDGE_WEIGHT_FORMAT:') or line.startswith('EDGE_WEIGHT_TYPE:'):\n",
    "                    self.edge_weight_format = line.split()[-1]  \n",
    "                elif line.startswith('NODE_COORD_SECTION'):\n",
    "                    # 读到不能再读坐标行为止\n",
    "                    line = f.readline().strip()\n",
    "                    while line:\n",
    "                        parts = line.split()\n",
    "                        if _is_coord_line(parts):\n",
    "                            self.node_coordinates[str(int(parts[0]) - 1)] = [float(parts[1]), float(parts[2])]\n",
    "                            line = f.readline().strip()\n",
    "                        else:\n",
    "                            # 遇到下一段的标题行（如 DEMAND_SECTION）或其它非坐标行\n",
    "                            break\n",
    "                    continue # 让外层 while 用当前的 line 继续判断下一段\n",
    "                elif line.startswith('DEMAND_SECTION'):\n",
    "                    demand_cnt = 0  # 统计 demand 行数\n",
    "\n",
    "                    # 同理：读到不能再读需求行为止\n",
    "                    line = f.readline().strip()\n",
    "                    while line:\n",
    "                        parts = line.split()\n",
    "                        if _is_demand_line(parts):\n",
    "                            self.demands[str(int(parts[0]) - 1)] = int(parts[1])\n",
    "                            demand_cnt += 1\n",
    "                            line = f.readline().strip()\n",
    "                        else:\n",
    "                            break\n",
    "                    \n",
    "                    # 对新老实例统一适用：customer 数 = demand 行数 - 1（去掉 depot）\n",
    "                    if demand_cnt > 0:\n",
    "                        self.num_of_customers = demand_cnt - 1\n",
    "                    else:\n",
    "                        raise Exception(\"Empty DEMAND_SECTION\")\n",
    "        \n",
    "                    continue           \n",
    "                elif line.startswith('STATIONS_COORD_SECTION'):\n",
    "                    for i in range(self.num_of_stations):\n",
    "                        self.station_list.append(str(int(f.readline().split()[0]) - 1))    \n",
    "                elif line.startswith('DEPOT_SECTION'):\n",
    "                    self.depot_index = str(int(f.readline().strip()) - 1)\n",
    "                line = f.readline().strip() \n",
    "        \n",
    "        # make some process the data\n",
    "        self.actual_problem_size = self.num_of_customers + self.num_of_stations + 1\n",
    "\n",
    "        \n",
    "        if self.edge_weight_format == 'EUC_2D':\n",
    "            node_coords = np.array(list(self.node_coordinates.values()))\n",
    "            self.distance_matrix = np.sqrt(np.sum((node_coords[:,np.newaxis] - node_coords)**2, axis=2))            \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported edge weight format: {}\".format(self.edge_weight_format))\n",
    "\n",
    "    def get_distance(self, node_1, node_2):\n",
    "        '''Get the Euclidean distance between two nodes.\n",
    "\n",
    "        Args:\n",
    "            node_1: a index of a node\n",
    "            node_2: a index of a node\n",
    "\n",
    "        Returns:\n",
    "            a real number denoting the distance  \n",
    "        '''\n",
    "        return self.distance_matrix[int(node_1)][int(node_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0f6cb7e-15a6-4d26-b4ca-db8ecb6eee6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/home/exx866/Instance-Aware-Algorithm-Configuration/data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = os.path.abspath(os.path.dirname(os.path.dirname('.')))\n",
    "data_dir = os.path.join(BASE_DIR, 'data')\n",
    "result_dir = os.path.join(BASE_DIR, 'data_results')\n",
    "\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46fe75cd-0d31-4a40-b4bf-40f0c55263b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = fnmatch.filter(os.listdir(data_dir), '*.evrp')\n",
    "# 提取 -nXXX- 中的数字并排序\n",
    "def extract_n_value(filename):\n",
    "    match = re.search(r'-n(\\d+)-', filename)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "sorted_filenames = sorted(filenames, key=extract_n_value)\n",
    "# sorted_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac1a169-d276-44b0-8594-713800d93e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(data_dir, \"E-n22-k4.evrp\")\n",
    "inst = Instance(filename)\n",
    "# inst.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c444830-fc53-418e-9ab6-2e5782beb4a2",
   "metadata": {},
   "source": [
    "## Graph/Topology features for E-CVRP instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b4c4e18-7502-4f5a-8ed3-3de8111cd57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(data_dir, \"X-n830-k171-s11.evrp\")\n",
    "inst = Instance(filename)\n",
    "# inst.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8fa4eb4-f8db-4649-951b-e3b9d10593ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'graph_kind': 'knn(k=10, mutual=True)', 'N_nodes': 830, 'depot_in_giant_component': True, 'is_connected': True, 'N_giant': 830, 'M_giant': 3152, 'deg_mean': 7.595180722891566, 'deg_std': 1.878213943602178, 'deg_min': 1.0, 'deg_max': 10.0, 'deg_gc_mean': 7.595180722891566, 'deg_gc_std': 1.878213943602178, 'edge_w_gc_mean': 26.48629332152885, 'edge_w_gc_std': 18.18425846532247, 'edge_w_gc_min': 1.0, 'edge_w_gc_max': 153.5838533179839, 'clust_gc_mean': 0.5908978772231784, 'clust_gc_std': 0.1558989098719094, 'mst_weight': 14383.32667646546, 'mst_weight_per_node': 17.35021311998246, 'mst_deg_max': 4.0, 'mst_deg_mean': 1.997590361445783, 'avg_shortest_path_w': 634.8333198756574, 'diameter_unweighted': 36, 'depot_betweenness_w': 0.37188743786530537, 'depot_closeness_w': 0.0021194509994602686, 'degree_assortativity': 0.4057535389842504, 'lap_eig_min': 2.1546358657722796e-16, 'lap_eig_max': 1.7935121683620046, 'lap_eig_mean': 1.0, 'lap_eig_std': 0.42441933199584864, 'algebraic_connectivity': 0.0001251768992344409, 'pairdist_mean': 536.291812020334, 'pairdist_std': 289.53556428487394, 'pairdist_cv': 0.5398843648090155, 'nn_dist_mean': 13.699835162424682, 'nn_dist_std': 13.23340282230281, 'nn_dist_cv': 0.9659534341404937, 'num_customers': 818, 'num_stations': 11, 'cust_to_station_nn_mean': 103.41707624109557, 'cust_to_station_nn_std': 37.31492264185843}\n",
      "This module provides compute_graph_features(instance, ...).\n",
      "Integrate it with your Instance class and call it per instance.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Graph/Topology features for EVRP instances using NetworkX.\n",
    "\n",
    "Assumptions (matching your Instance reader):\n",
    "- instance.node_coordinates: dict[str, [x, y]] for ALL nodes (customers + stations), indexed from 0\n",
    "- instance.demands: dict[str, demand] for customers only\n",
    "- instance.station_list: list[str] of station node indices (0-based as strings)\n",
    "- instance.depot_index: str of depot node index (0-based)\n",
    "- instance.actual_problem_size = dimension + num_of_stations\n",
    "- instance.distance_matrix: NxN numpy array of Euclidean distances\n",
    "\n",
    "We build a sparse graph (kNN) to compute topology features efficiently.\n",
    "We also compute MST on the kNN graph (or fallback to full metric if needed).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "\n",
    "def _safe_float(x: float) -> float:\n",
    "    # ensure JSON-serializable floats (avoid numpy types)\n",
    "    return float(x)\n",
    "\n",
    "\n",
    "def _clip_int(x: int, lo: int, hi: int) -> int:\n",
    "    return max(lo, min(hi, x))\n",
    "\n",
    "\n",
    "def _as_int_ids(instance) -> List[int]:\n",
    "    # node ids are 0..N-1\n",
    "    return list(range(int(instance.actual_problem_size)))\n",
    "\n",
    "\n",
    "def _coord_array(instance) -> np.ndarray:\n",
    "    # node_coordinates is dict[str, [x,y]]\n",
    "    N = int(instance.actual_problem_size)\n",
    "    coords = np.zeros((N, 2), dtype=float)\n",
    "    for i in range(N):\n",
    "        coords[i, :] = instance.node_coordinates[str(i)]\n",
    "    return coords\n",
    "\n",
    "\n",
    "def _node_types(instance) -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    Return node type: 'depot', 'customer', 'station'\n",
    "    \"\"\"\n",
    "    depot = int(instance.depot_index)\n",
    "    stations = set(int(s) for s in instance.station_list)\n",
    "    types: Dict[int, str] = {}\n",
    "    N = int(instance.actual_problem_size)\n",
    "    for i in range(N):\n",
    "        if i == depot:\n",
    "            types[i] = \"depot\"\n",
    "        elif i in stations:\n",
    "            types[i] = \"station\"\n",
    "        else:\n",
    "            types[i] = \"customer\"\n",
    "    return types\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Graph construction\n",
    "# -----------------------------\n",
    "\n",
    "def build_knn_graph(instance, k: int = 10, mutual: bool = True) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Build a weighted kNN graph using Euclidean distances from instance.distance_matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k : number of nearest neighbors per node (clipped to [1, N-1])\n",
    "    mutual : if True, keep an undirected edge (i,j) only if i is in kNN(j) AND j in kNN(i)\n",
    "             if False, use the union (i in kNN(j) OR j in kNN(i))\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    G : networkx.Graph with:\n",
    "        - nodes: 0..N-1\n",
    "        - edge attribute 'weight' = Euclidean distance\n",
    "    \"\"\"\n",
    "    N = int(instance.actual_problem_size)\n",
    "    if N < 2:\n",
    "        raise ValueError(\"Instance must have at least 2 nodes to build a graph.\")\n",
    "    k = _clip_int(int(k), 1, N - 1)\n",
    "\n",
    "    D = instance.distance_matrix  # NxN numpy array\n",
    "    # For each i, find k nearest j != i\n",
    "    knn = []\n",
    "    for i in range(N):\n",
    "        row = D[i].copy()\n",
    "        row[i] = np.inf\n",
    "        # nbrs = np.argpartition(row, k)[:k]\n",
    "        nbrs = np.argpartition(row, k - 1)[:k]\n",
    "        nbrs = nbrs[np.argsort(row[nbrs])]  # sort by distance\n",
    "        knn.append(set(int(j) for j in nbrs))\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(N))\n",
    "\n",
    "    if mutual:\n",
    "        for i in range(N):\n",
    "            for j in knn[i]:\n",
    "                if i in knn[j]:\n",
    "                    w = float(D[i][j])\n",
    "                    if i != j:\n",
    "                        G.add_edge(i, j, weight=w)\n",
    "    else:\n",
    "        for i in range(N):\n",
    "            for j in knn[i]:\n",
    "                w = float(D[i][j])\n",
    "                if i != j:\n",
    "                    G.add_edge(i, j, weight=w)\n",
    "\n",
    "    # ensure connectedness if possible: if disconnected, we will keep it but\n",
    "    # downstream features will compute on giant component where needed.\n",
    "    return G\n",
    "\n",
    "\n",
    "def build_complete_graph(instance) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Build a complete weighted graph (O(N^2) edges). Only use for small instances.\n",
    "    \"\"\"\n",
    "    N = int(instance.actual_problem_size)\n",
    "    D = instance.distance_matrix\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(N))\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N):\n",
    "            G.add_edge(i, j, weight=float(D[i][j]))\n",
    "    return G\n",
    "\n",
    "\n",
    "def ensure_connected_graph(G: nx.Graph) -> Tuple[nx.Graph, bool]:\n",
    "    \"\"\"\n",
    "    Return (Gc, was_connected)\n",
    "    - If G connected: return (G, True)\n",
    "    - Else: return (largest connected component induced subgraph, False)\n",
    "    \"\"\"\n",
    "    if nx.is_connected(G):\n",
    "        return G, True\n",
    "    comps = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "    giant = comps[0]\n",
    "    return G.subgraph(giant).copy(), False\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Feature extraction\n",
    "# -----------------------------\n",
    "\n",
    "def compute_graph_features(\n",
    "    instance,\n",
    "    *,\n",
    "    k: int = 10,\n",
    "    mutual_knn: bool = True,\n",
    "    use_complete_graph_if_small: bool = False,\n",
    "    complete_graph_threshold: int = 10,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compute topology/graph features for a given instance.\n",
    "\n",
    "    Strategy:\n",
    "    - Build kNN graph (or complete graph if N <= threshold and enabled).\n",
    "    - Use giant component for path-based statistics if graph disconnected.\n",
    "    - Compute MST weight (over the chosen graph). If graph disconnected, MST is for giant component.\n",
    "    - Compute selected centralities and clustering stats.\n",
    "    - Compute spectral features of the normalized Laplacian on the giant component.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features : dict[str, Any]\n",
    "    \"\"\"\n",
    "    N = int(instance.actual_problem_size)\n",
    "    depot = int(instance.depot_index)\n",
    "    node_types = _node_types(instance)\n",
    "\n",
    "    # Choose graph\n",
    "    if use_complete_graph_if_small and N <= complete_graph_threshold:\n",
    "        G = build_complete_graph(instance)\n",
    "        graph_kind = \"complete\"\n",
    "    else:\n",
    "        G = build_knn_graph(instance, k=k, mutual=mutual_knn)\n",
    "        graph_kind = f\"knn(k={_clip_int(k,1,max(1,N-1))}, mutual={mutual_knn})\"\n",
    "\n",
    "    # Connectivity\n",
    "    Gc, is_conn = ensure_connected_graph(G)\n",
    "    n_gc = Gc.number_of_nodes()\n",
    "    m_gc = Gc.number_of_edges()\n",
    "\n",
    "    # Basic stats\n",
    "    degrees = np.array([d for _, d in G.degree()], dtype=float)\n",
    "    degrees_gc = np.array([d for _, d in Gc.degree()], dtype=float)\n",
    "\n",
    "    # Weighted edges\n",
    "    weights = np.array([edata[\"weight\"] for _, _, edata in G.edges(data=True)], dtype=float) if G.number_of_edges() else np.array([])\n",
    "    weights_gc = np.array([edata[\"weight\"] for _, _, edata in Gc.edges(data=True)], dtype=float) if m_gc else np.array([])\n",
    "\n",
    "    # MST (on Gc)\n",
    "    mst = nx.minimum_spanning_tree(Gc, weight=\"weight\")\n",
    "    mst_weight = sum(edata[\"weight\"] for _, _, edata in mst.edges(data=True)) if mst.number_of_edges() else 0.0\n",
    "    mst_degrees = np.array([d for _, d in mst.degree()], dtype=float) if mst.number_of_nodes() else np.array([0.0])\n",
    "\n",
    "    # Clustering (unweighted clustering; weighted clustering exists but is slower)\n",
    "    clustering_gc = nx.clustering(Gc)\n",
    "    clustering_vals = np.array(list(clustering_gc.values()), dtype=float) if clustering_gc else np.array([0.0])\n",
    "\n",
    "    # Path-based features (on giant component only)\n",
    "    # For kNN graphs, shortest paths exist if connected; if not, we use giant component.\n",
    "    # Weighted shortest path length:\n",
    "    try:\n",
    "        apl_w = nx.average_shortest_path_length(Gc, weight=\"weight\") if n_gc > 1 else 0.0\n",
    "    except Exception:\n",
    "        apl_w = float(\"nan\")\n",
    "\n",
    "    # Unweighted diameter / eccentricity\n",
    "    try:\n",
    "        diameter = nx.diameter(Gc) if n_gc > 1 else 0\n",
    "    except Exception:\n",
    "        diameter = None\n",
    "\n",
    "    # Depot centrality in Gc (if depot outside giant component, set NaN)\n",
    "    depot_in_gc = depot in Gc.nodes\n",
    "    if depot_in_gc and n_gc > 1:\n",
    "        # betweenness can be expensive; on big graphs consider approximation\n",
    "        betw = nx.betweenness_centrality(Gc, k=None, normalized=True, weight=\"weight\")\n",
    "        clos = nx.closeness_centrality(Gc, distance=\"weight\")\n",
    "        dep_betw = float(betw.get(depot, float(\"nan\")))\n",
    "        dep_clos = float(clos.get(depot, float(\"nan\")))\n",
    "    else:\n",
    "        dep_betw = float(\"nan\")\n",
    "        dep_clos = float(\"nan\")\n",
    "\n",
    "    # Degree assortativity (may be NaN for small graphs)\n",
    "    try:\n",
    "        assort = nx.degree_assortativity_coefficient(Gc)\n",
    "    except Exception:\n",
    "        assort = float(\"nan\")\n",
    "\n",
    "    # Spectral features (normalized Laplacian eigenvalues)\n",
    "    # Use only small/medium giant component to keep it safe; for huge graphs, take k smallest eigenvalues via sparse methods (not used here).\n",
    "    spectral = {}\n",
    "    if n_gc <= 1500 and n_gc > 2:\n",
    "        try:\n",
    "            Gc_sim = nx.Graph()\n",
    "            Gc_sim.add_nodes_from(Gc.nodes())\n",
    "            eps = 1e-9\n",
    "            for u, v, edata in Gc.edges(data=True):\n",
    "                d = float(edata[\"weight\"])\n",
    "                Gc_sim.add_edge(u, v, weight=1.0/(d+eps))\n",
    "            \n",
    "            L = nx.normalized_laplacian_matrix(Gc_sim, weight=\"weight\").astype(float)\n",
    "            # L = nx.normalized_laplacian_matrix(Gc, weight=\"weight\").astype(float)\n",
    "            # Convert to dense for eigvalsh when size manageable\n",
    "            Ld = L.toarray()\n",
    "            evals = np.linalg.eigvalsh(Ld)\n",
    "            # summary stats\n",
    "            spectral[\"lap_eig_min\"] = _safe_float(np.min(evals))\n",
    "            spectral[\"lap_eig_max\"] = _safe_float(np.max(evals))\n",
    "            spectral[\"lap_eig_mean\"] = _safe_float(np.mean(evals))\n",
    "            spectral[\"lap_eig_std\"] = _safe_float(np.std(evals))\n",
    "            # algebraic connectivity = second smallest eigenvalue (Fiedler value)\n",
    "            spectral[\"algebraic_connectivity\"] = _safe_float(np.sort(evals)[1])\n",
    "        except Exception:\n",
    "            spectral[\"lap_eig_min\"] = float(\"nan\")\n",
    "            spectral[\"lap_eig_max\"] = float(\"nan\")\n",
    "            spectral[\"lap_eig_mean\"] = float(\"nan\")\n",
    "            spectral[\"lap_eig_std\"] = float(\"nan\")\n",
    "            spectral[\"algebraic_connectivity\"] = float(\"nan\")\n",
    "    else:\n",
    "        spectral[\"lap_eig_min\"] = float(\"nan\")\n",
    "        spectral[\"lap_eig_max\"] = float(\"nan\")\n",
    "        spectral[\"lap_eig_mean\"] = float(\"nan\")\n",
    "        spectral[\"lap_eig_std\"] = float(\"nan\")\n",
    "        spectral[\"algebraic_connectivity\"] = float(\"nan\")\n",
    "\n",
    "    # Spatial/geometric quick extras (helpful alongside graph features)\n",
    "    coords = _coord_array(instance)\n",
    "    # pairwise distances summary via distance matrix (avoid O(N^2) if N huge)\n",
    "    # Here we sample pairs for large N\n",
    "    D = instance.distance_matrix\n",
    "    if N <= 800:\n",
    "        upper = D[np.triu_indices(N, k=1)]\n",
    "    else:\n",
    "        rng = np.random.default_rng(0)\n",
    "        idx_i = rng.integers(0, N, size=20000)\n",
    "        idx_j = rng.integers(0, N, size=20000)\n",
    "        mask = idx_i != idx_j\n",
    "        upper = D[idx_i[mask], idx_j[mask]]\n",
    "\n",
    "    # nearest neighbor distance per node\n",
    "    nn = []\n",
    "    for i in range(N):\n",
    "        row = D[i].copy()\n",
    "        row[i] = np.inf\n",
    "        nn.append(float(np.min(row)))\n",
    "    nn = np.array(nn, dtype=float)\n",
    "\n",
    "    features: Dict[str, Any] = {\n",
    "        # meta\n",
    "        \"graph_kind\": graph_kind,\n",
    "        \"N_nodes\": N,\n",
    "        \"depot_in_giant_component\": bool(depot_in_gc),\n",
    "        \"is_connected\": bool(is_conn),\n",
    "        \"N_giant\": int(n_gc),\n",
    "        \"M_giant\": int(m_gc),\n",
    "\n",
    "        # degree stats (whole graph)\n",
    "        \"deg_mean\": _safe_float(np.mean(degrees)) if len(degrees) else 0.0,\n",
    "        \"deg_std\": _safe_float(np.std(degrees)) if len(degrees) else 0.0,\n",
    "        \"deg_min\": _safe_float(np.min(degrees)) if len(degrees) else 0.0,\n",
    "        \"deg_max\": _safe_float(np.max(degrees)) if len(degrees) else 0.0,\n",
    "\n",
    "        # degree stats (giant component)\n",
    "        \"deg_gc_mean\": _safe_float(np.mean(degrees_gc)) if len(degrees_gc) else 0.0,\n",
    "        \"deg_gc_std\": _safe_float(np.std(degrees_gc)) if len(degrees_gc) else 0.0,\n",
    "\n",
    "        # edge weight stats (giant component)\n",
    "        \"edge_w_gc_mean\": _safe_float(np.mean(weights_gc)) if len(weights_gc) else 0.0,\n",
    "        \"edge_w_gc_std\": _safe_float(np.std(weights_gc)) if len(weights_gc) else 0.0,\n",
    "        \"edge_w_gc_min\": _safe_float(np.min(weights_gc)) if len(weights_gc) else 0.0,\n",
    "        \"edge_w_gc_max\": _safe_float(np.max(weights_gc)) if len(weights_gc) else 0.0,\n",
    "\n",
    "        # clustering\n",
    "        \"clust_gc_mean\": _safe_float(np.mean(clustering_vals)) if len(clustering_vals) else 0.0,\n",
    "        \"clust_gc_std\": _safe_float(np.std(clustering_vals)) if len(clustering_vals) else 0.0,\n",
    "\n",
    "        # MST\n",
    "        \"mst_weight\": _safe_float(mst_weight),\n",
    "        \"mst_weight_per_node\": _safe_float(mst_weight / max(1, n_gc - 1)),\n",
    "        \"mst_deg_max\": _safe_float(np.max(mst_degrees)) if len(mst_degrees) else 0.0,\n",
    "        \"mst_deg_mean\": _safe_float(np.mean(mst_degrees)) if len(mst_degrees) else 0.0,\n",
    "\n",
    "        # shortest paths / diameter\n",
    "        \"avg_shortest_path_w\": _safe_float(apl_w) if apl_w == apl_w else float(\"nan\"),\n",
    "        \"diameter_unweighted\": int(diameter) if diameter is not None else None,\n",
    "\n",
    "        # centrality (depot)\n",
    "        \"depot_betweenness_w\": _safe_float(dep_betw) if dep_betw == dep_betw else float(\"nan\"),\n",
    "        \"depot_closeness_w\": _safe_float(dep_clos) if dep_clos == dep_clos else float(\"nan\"),\n",
    "\n",
    "        # assortativity\n",
    "        \"degree_assortativity\": _safe_float(assort) if assort == assort else float(\"nan\"),\n",
    "\n",
    "        # spectral\n",
    "        **spectral,\n",
    "\n",
    "        # geometric summaries (distance matrix based)\n",
    "        \"pairdist_mean\": _safe_float(np.mean(upper)) if len(upper) else 0.0,\n",
    "        \"pairdist_std\": _safe_float(np.std(upper)) if len(upper) else 0.0,\n",
    "        \"pairdist_cv\": _safe_float(np.std(upper) / np.mean(upper)) if len(upper) and np.mean(upper) > 0 else 0.0,\n",
    "        \"nn_dist_mean\": _safe_float(np.mean(nn)) if len(nn) else 0.0,\n",
    "        \"nn_dist_std\": _safe_float(np.std(nn)) if len(nn) else 0.0,\n",
    "        \"nn_dist_cv\": _safe_float(np.std(nn) / np.mean(nn)) if len(nn) and np.mean(nn) > 0 else 0.0,\n",
    "    }\n",
    "\n",
    "    # Optional: type-based stats (customers vs stations)\n",
    "    types = _node_types(instance)\n",
    "    customers = [i for i, t in types.items() if t == \"customer\"]\n",
    "    stations = [i for i, t in types.items() if t == \"station\"]\n",
    "    features[\"num_customers\"] = len(customers)\n",
    "    features[\"num_stations\"] = len(stations)\n",
    "\n",
    "    # average customer->nearest station distance (use distance matrix)\n",
    "    if stations and customers:\n",
    "        dcs = []\n",
    "        for c in customers:\n",
    "            dcs.append(float(np.min(D[c, stations])))\n",
    "        dcs = np.array(dcs, dtype=float)\n",
    "        features[\"cust_to_station_nn_mean\"] = _safe_float(np.mean(dcs))\n",
    "        features[\"cust_to_station_nn_std\"] = _safe_float(np.std(dcs))\n",
    "    else:\n",
    "        features[\"cust_to_station_nn_mean\"] = float(\"nan\")\n",
    "        features[\"cust_to_station_nn_std\"] = float(\"nan\")\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example (adapt to your project):\n",
    "    # from your_instance_reader import Instance\n",
    "    inst = Instance(filename)\n",
    "    feats = compute_graph_features(inst, k=10, mutual_knn=True)\n",
    "    print(feats)\n",
    "\n",
    "    print(\"This module provides compute_graph_features(instance, ...).\")\n",
    "    print(\"Integrate it with your Instance class and call it per instance.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8e7d32-a1cf-4b85-8a8e-49bd4e3f1245",
   "metadata": {},
   "source": [
    "{'graph_kind': 'complete', 'N_nodes': 830, 'depot_in_giant_component': True, 'is_connected': True, 'N_giant': 830, 'M_giant': 344035, 'deg_mean': 829.0, 'deg_std': 0.0, 'deg_min': 829.0, 'deg_max': 829.0, 'deg_gc_mean': 829.0, 'deg_gc_std': 0.0, 'edge_w_gc_mean': 532.4557328489265, 'edge_w_gc_std': 288.29913651109644, 'edge_w_gc_min': 1.0, 'edge_w_gc_max': 1339.334909572658, 'clust_gc_mean': 1.0, 'clust_gc_std': 0.0, 'mst_weight': 14335.685072470109, 'mst_weight_per_node': 17.292744357623775, 'mst_deg_max': 4.0, 'mst_deg_mean': 1.997590361445783, 'avg_shortest_path_w': 532.4557328489141, 'diameter_unweighted': 1, 'depot_betweenness_w': 0.0, 'depot_closeness_w': 0.0023443289774536745, 'degree_assortativity': nan, 'lap_eig_min': nan, 'lap_eig_max': nan, 'lap_eig_mean': nan, 'lap_eig_std': nan, 'algebraic_connectivity': nan, 'pairdist_mean': 536.291812020334, 'pairdist_std': 289.53556428487394, 'pairdist_cv': 0.5398843648090155, 'nn_dist_mean': 13.699835162424682, 'nn_dist_std': 13.23340282230281, 'nn_dist_cv': 0.9659534341404937, 'num_customers': 818, 'num_stations': 11, 'cust_to_station_nn_mean': 103.41707624109557, 'cust_to_station_nn_std': 37.31492264185843}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e99eac6-d584-437a-9b58-7731ff58993a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4393f840-d80a-4c4e-8308-95645511bae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2b9d6b7-fafa-4f95-9f6d-e547bff51794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "import pdb\n",
    "\n",
    "def compute_demand_features(instance):\n",
    "    Q = float(instance.capacity) if instance.capacity is not None else 0.0\n",
    "    depot = int(instance.depot_index) if instance.depot_index is not None else None\n",
    "\n",
    "    # 只取 customers：demands 里去掉 depot\n",
    "    keys = []\n",
    "    for k in instance.demands.keys():\n",
    "        ik = int(k)\n",
    "        if depot is not None and ik == depot:\n",
    "            continue\n",
    "        keys.append(ik)\n",
    "    keys.sort()\n",
    "\n",
    "    demands = np.array([float(instance.demands[str(k)]) for k in keys], dtype=float)\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    if demands.size == 0:\n",
    "        # 没有 customer 的极端情况\n",
    "        feats.update({\n",
    "            \"demand_mean\": 0.0,\n",
    "            \"demand_std\": 0.0,\n",
    "            \"demand_cv\": 0.0,\n",
    "            \"demand_to_capacity_mean\": np.nan,\n",
    "            \"demand_to_capacity_max\": np.nan,\n",
    "            \"total_demand_to_capacity\": np.nan,\n",
    "            \"demand_skewness\": 0.0,\n",
    "            \"demand_entropy\": 0.0,\n",
    "            \"high_demand_ratio\": 0.0,\n",
    "            \"demand_weighted_station_dist\": np.nan,\n",
    "        })\n",
    "        return feats\n",
    "\n",
    "    # basic stats\n",
    "    mu = float(np.mean(demands))\n",
    "    sd = float(np.std(demands))\n",
    "    feats[\"demand_mean\"] = mu\n",
    "    feats[\"demand_std\"] = sd\n",
    "    feats[\"demand_cv\"] = float(sd / mu) if mu > 0 else 0.0\n",
    "\n",
    "    # capacity tightness\n",
    "    if Q > 0:\n",
    "        feats[\"demand_to_capacity_mean\"] = float(mu / Q)\n",
    "        feats[\"demand_to_capacity_max\"] = float(np.max(demands) / Q)\n",
    "        feats[\"total_demand_to_capacity\"] = float(np.sum(demands) / Q)\n",
    "    else:\n",
    "        feats[\"demand_to_capacity_mean\"] = np.nan\n",
    "        feats[\"demand_to_capacity_max\"] = np.nan\n",
    "        feats[\"total_demand_to_capacity\"] = np.nan\n",
    "\n",
    "    # distribution shape\n",
    "    feats[\"demand_skewness\"] = float(skew(demands)) if demands.size >= 3 else 0.0\n",
    "\n",
    "    # entropy (use probabilities, not density)\n",
    "    counts, _ = np.histogram(demands, bins=\"auto\", density=False)\n",
    "    counts = counts[counts > 0]\n",
    "    p = counts / counts.sum()\n",
    "    feats[\"demand_entropy\"] = float(-np.sum(p * np.log(p)))\n",
    "\n",
    "    # high demand ratio\n",
    "    alpha = 0.5\n",
    "    feats[\"high_demand_ratio\"] = float(np.mean(demands > alpha * Q)) if Q > 0 else np.nan\n",
    "\n",
    "    # demand-weighted distance to nearest station\n",
    "    stations = [int(s) for s in instance.station_list] if instance.station_list is not None else []\n",
    "    D = instance.distance_matrix\n",
    "    if D is None or len(stations) == 0 or np.sum(demands) <= 0:\n",
    "        feats[\"demand_weighted_station_dist\"] = np.nan\n",
    "    else:\n",
    "        # keys 和 demands 对齐，直接 zip\n",
    "        weighted = []\n",
    "        for idx, d in zip(keys, demands):\n",
    "            dist = float(np.min(D[idx, stations]))\n",
    "            weighted.append(d * dist)\n",
    "        feats[\"demand_weighted_station_dist\"] = float(np.sum(weighted) / np.sum(demands))\n",
    "\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fff7bf3b-d1d3-4232-87e4-f908bdb30ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'demand_mean': 74.46699266503667, 'demand_std': 14.484694257337729, 'demand_cv': 0.194511604926655, 'demand_to_capacity_mean': 0.20800835939954376, 'demand_to_capacity_max': 0.27932960893854747, 'total_demand_to_capacity': 170.15083798882682, 'demand_skewness': 0.05309679003134673, 'demand_entropy': 2.3902902988690244, 'high_demand_ratio': 0.0, 'demand_weighted_station_dist': 103.36038564330435}\n"
     ]
    }
   ],
   "source": [
    "feats = compute_demand_features(inst)\n",
    "print(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e39ae0f-6d3d-43cd-9335-e76b772dd227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing E-n22-k4.evrp ...\n",
      "Processing E-n23-k3.evrp ...\n",
      "Processing E-n29-k4-s7.evrp ...\n",
      "Processing E-n30-k3.evrp ...\n",
      "Processing E-n30-k3-s7.evrp ...\n",
      "Processing E-n33-k4.evrp ...\n",
      "Processing E-n35-k3-s5.evrp ...\n",
      "Processing E-n37-k4-s4.evrp ...\n",
      "Processing F-n49-k4-s4.evrp ...\n",
      "Processing E-n51-k5.evrp ...\n",
      "Processing E-n60-k5-s9.evrp ...\n",
      "Processing E-n76-k7.evrp ...\n",
      "Processing F-n80-k4-s8.evrp ...\n",
      "Processing E-n89-k7-s13.evrp ...\n",
      "Processing E-n101-k8.evrp ...\n",
      "Processing M-n110-k10-s9.evrp ...\n",
      "Processing E-n112-k8-s11.evrp ...\n",
      "Processing M-n126-k7-s5.evrp ...\n",
      "Processing F-n140-k5-s5.evrp ...\n",
      "Processing X-n143-k7.evrp ...\n",
      "Processing X-n147-k7-s4.evrp ...\n",
      "Processing M-n163-k12-s12.evrp ...\n",
      "Processing M-n212-k16-s12.evrp ...\n",
      "Processing X-n214-k11.evrp ...\n",
      "Processing X-n221-k11-s7.evrp ...\n",
      "Processing X-n351-k40.evrp ...\n",
      "Processing X-n360-k40-s9.evrp ...\n",
      "Processing X-n459-k26.evrp ...\n",
      "Processing X-n469-k26-s10.evrp ...\n",
      "Processing X-n573-k30.evrp ...\n",
      "Processing X-n577-k30-s4.evrp ...\n",
      "Processing X-n685-k75.evrp ...\n",
      "Processing X-n698-k75-s13.evrp ...\n",
      "Processing X-n749-k98.evrp ...\n",
      "Processing X-n759-k98-s10.evrp ...\n",
      "Processing X-n819-k171.evrp ...\n",
      "Processing X-n830-k171-s11.evrp ...\n",
      "Processing X-n916-k207.evrp ...\n",
      "Processing X-n920-k207-s4.evrp ...\n",
      "Processing X-n1001-k43.evrp ...\n",
      "Processing X-n1006-k43-s5.evrp ...\n",
      "\n",
      "✅ Feature table saved to: /data/home/exx866/Instance-Aware-Algorithm-Configuration/data/evrp_instance_features.csv\n",
      "(41, 61)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "records = []\n",
    "\n",
    "for fname in sorted_filenames:\n",
    "    path = os.path.join(data_dir, fname)\n",
    "    print(f\"Processing {fname} ...\")\n",
    "\n",
    "    try:\n",
    "        inst = Instance(path)\n",
    "\n",
    "        graph_feats = compute_graph_features(inst)\n",
    "        demand_feats = compute_demand_features(inst)\n",
    "\n",
    "        # 合并所有特征\n",
    "        feats = {}\n",
    "        feats.update(graph_feats)\n",
    "        feats.update(demand_feats)\n",
    "\n",
    "        # 加一些元信息（非常有用）\n",
    "        feats[\"instance_name\"] = inst.name\n",
    "        feats[\"filename\"] = fname\n",
    "        feats[\"N_total\"] = inst.actual_problem_size\n",
    "        feats[\"num_customers\"] = inst.num_of_customers\n",
    "        feats[\"num_stations\"] = len(inst.station_list)\n",
    "        feats[\"num_depot\"] = 1\n",
    "        feats[\"vehicle_capacity\"] = inst.capacity\n",
    "        feats[\"battery_capacity\"] = inst.battery_capacity\n",
    "        feats[\"energy_consumption\"] = inst.energy_consumption\n",
    "        feats[\"num_vehicles\"] = inst.num_of_vehicles\n",
    "        feats[\"max_evals\"] = inst.actual_problem_size * 25000\n",
    "        \n",
    "\n",
    "        records.append(feats)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed on {fname}: {e}\")\n",
    "\n",
    "\n",
    "# 汇总为 DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# 想放前面的“元信息列”（用你真实的字段名）\n",
    "meta_cols = [\n",
    "    \"instance_name\",\n",
    "    \"filename\",  # 建议保留\n",
    "    \"graph_kind\",\n",
    "    \"N_total\",\n",
    "    \"num_customers\",\n",
    "    \"num_stations\",\n",
    "    \"num_depot\",\n",
    "    \"num_vehicles\",\n",
    "    \"vehicle_capacity\",\n",
    "    \"battery_capacity\",\n",
    "    \"energy_consumption\",\n",
    "    \"max_evals\",\n",
    "]\n",
    "\n",
    "# 只选择 реально存在的列，避免 KeyError\n",
    "meta_cols_present = [c for c in meta_cols if c in df.columns]\n",
    "other_cols = [c for c in df.columns if c not in meta_cols_present]\n",
    "df = df[meta_cols_present + other_cols]\n",
    "\n",
    "# 保存为 CSV\n",
    "out_csv = os.path.join(data_dir, \"evrp_instance_features.csv\")\n",
    "df.to_csv(out_csv, index=False)\n",
    "\n",
    "print(f\"\\n✅ Feature table saved to: {out_csv}\")\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202fe852-832c-4cbd-b8b6-fe248ae14578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07fb9c7-24ad-45bd-ba1d-12a40c7e945d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28195db5-f5a2-4193-841f-d0e28aa0749e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a6979c-a2c5-42ba-9737-8f68fb599543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blue_sky",
   "language": "python",
   "name": "blue_sky"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
